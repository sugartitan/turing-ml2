# Stroke Prediction Data Aalysis
2nd Project for Turing ML Module

## About this Part
Congrats! You have reached the last Part of this Sprint. In this Part, you will put what you learned during this and the previous Sprints into practice. As the final assignment of this Sprint, you will train and deploy a machine learning model on the Stroke Prediction Dataset. You will have to apply all that you have learned about training and deploying machine learning models to complete this task. Our expectation is that you'll use your own judgment on how to perform the analysis and how to select the most important avenues of modeling, statistical testing, and exploration. You'll have to iteratively try to find patterns in the data, raise hypotheses and use your data analysis skills to get answers.

P.S. we don't expect this project to be perfect - you will continue to improve your skills, and there will be many projects for you to apply your newly gained skills in the future. For now, just use what you have learned and try your best!

## Context
Imagine that you are a data analyst working for The Johns Hopkins Hospital. Your team is asked to create a machine learning model, which could predict if the patient is likely to get a stroke - being able to determine which patients have high stroke risk will allow your doctors to advise them and their families on how to act in case of an emergency.

## Objectives for this Part
* Practice working with CSV files.
* Practice performing EDA.
* Practice applying statistical inference procedures.
* Practice using various types of machine learning models.
* Practice building ensembles of machine learning models.
* Practice deploying machine learning models.
* Practice visualizing data with Matplotlib & Seaborn.
* Practice reading data, performing queries, and filtering data.
## Requirements
* Download the data from Stroke Prediction Dataset.
* Perform exploratory data analysis. This should include creating statistical summaries and charts, testing for anomalies, and checking for correlations and other relations between variables and other EDA elements.
* Perform statistical inference. This should include defining the target population, forming multiple statistical hypotheses and constructing confidence intervals, setting the significance levels, and conducting z or t-tests for these hypotheses.
* Apply various machine learning models to predict the "stroke" column using all other features. This should include hyperparameter tuning, model ensembling, the analysis of model selection, and other methods. Suggestion: you might want to investigate how to use (scikit-learn pipelines)[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html] to make your training pipelines more robust.
* Deploy the machine learning model. Choose the best performing model and deploy it. You are free to choose any deployment option that you like - you can deploy your model in a container (on your computer or on a server), do a serverless deployment on the cloud, or even deploy and serve it on the browser as a web app.
* Provide clear explanations in your notebook. Your explanations should inform the reader what you are trying to achieve, what results did you get, and what these results mean.
* Provide suggestions about how your analysis can be improved.
## Evaluation Criteria
* Adherence to the requirements. How well did you meet the requirements?
* Depth of your analysis. Did you just skim the surface, or did you explore the dataset in-depth?
* Model's performance. How well did your model perform the predictions?
* Visualization quality. Did you use charts effectively to visualize patterns in the data? Are your visualizations properly labeled? Did you use colors effectively? Did you adhere to the principle of proportional ink?
* Code quality. Was your code well-structured? Did you use the appropriate levels of abstraction? Did you remove commented-out and unused code? Did you adhere to the PEP8?
* Code performance. Did you use suitable algorithms and data structures to solve the problems?
